{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import basic libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set basic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 128\n",
    "EPOCH = 1\n",
    "LATENT_DIM = 500\n",
    "LATENT_DIM_DECODER = LATENT_DIM\n",
    "SAMPLES = 23000\n",
    "MAX_WORD_NUM = SAMPLES\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING = MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store data from textfile to usable arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Count: 2000.\n",
      "Sample Count: 4000.\n",
      "Sample Count: 6000.\n",
      "Sample Count: 8000.\n",
      "Sample Count: 10000.\n",
      "Sample Count: 12000.\n",
      "Sample Count: 14000.\n",
      "Sample Count: 16000.\n",
      "Sample Count: 18000.\n",
      "Sample Count: 20000.\n",
      "Sample Count: 22000.\n"
     ]
    }
   ],
   "source": [
    "eng = []\n",
    "man = []\n",
    "man_inputs = []\n",
    "count = 0\n",
    "\n",
    "# preprocess the translation file\n",
    "for line in open('Mandarin.txt'):\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "    \n",
    "    count += 1\n",
    "    if (count > SAMPLES):\n",
    "        break\n",
    "    \n",
    "    # split original and translation into lists\n",
    "    e, m, _ = line.rstrip().split('\\t')\n",
    "    eng.append(e)\n",
    "    man.append(m + ' <eos>')\n",
    "    man_inputs.append('<sos> ' + m)\n",
    "    \n",
    "    if (count % 2000 == 0):\n",
    "        print ('Sample Count: {}.'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the input and output sentences, and create maps that can be used by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input tokens: 6377\n",
      "Maximum input sequence length: 14\n",
      "Number of output tokens: 21329\n",
      "Maximum output sequence length: 4\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tokenize input and generate idx map\n",
    "tok_in = Tokenizer(num_words=MAX_WORD_NUM)\n",
    "tok_in.fit_on_texts(eng)\n",
    "eng_seq = tok_in.texts_to_sequences(eng)\n",
    "word2idx_in = tok_in.word_index\n",
    "max_in_len = max(len(s) for s in eng_seq)\n",
    "\n",
    "print(\"Number of input tokens: {}\".format(len(word2idx_in)))\n",
    "print(\"Maximum input sequence length: {}\".format(max_in_len))\n",
    "\n",
    "# tokenize output and generate idx map\n",
    "tok_out = Tokenizer(num_words=MAX_WORD_NUM, filters='')\n",
    "tok_out.fit_on_texts(man + man_inputs)\n",
    "man_seq = tok_out.texts_to_sequences(man)\n",
    "man_seq_inputs = tok_out.texts_to_sequences(man_inputs)\n",
    "word2idx_out = tok_out.word_index\n",
    "max_out_len = max(len(s) for s in man_seq)\n",
    "out_word_num = len(word2idx_out) + 1\n",
    "\n",
    "print(\"Number of output tokens: {}\".format(len(word2idx_out)))\n",
    "print(\"Maximum output sequence length: {}\".format(max_out_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the input and output sequences to be the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "eng_seq_padded = pad_sequences(eng_seq, maxlen=max_in_len)\n",
    "man_seq_padded = pad_sequences(man_seq, maxlen=max_out_len, padding='post')\n",
    "man_seq_inputs_padded = pad_sequences(man_seq_inputs, maxlen=max_out_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in word vectors and use them to create word embeddings. [The dataset](https://nlp.stanford.edu/data/glove.6B.zip) of the word vectors is downloaded from *nlp.stanford.edu*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wordVec\n",
      "Finished loading wordVec.\n"
     ]
    }
   ],
   "source": [
    "wordVec = {}\n",
    "\n",
    "print('Loading wordVec')\n",
    "\n",
    "# load in word vectors in a dict\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        data = line.split()\n",
    "        word = data[0]\n",
    "        vec = np.asarray(data[1:], dtype='float32')\n",
    "        wordVec[word] = vec\n",
    "\n",
    "print('Finished loading wordVec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordNum = min(MAX_WORD_NUM, len(word2idx_in) + 1)\n",
    "word_embedding = np.zeros((wordNum, EMBEDDING))\n",
    "\n",
    "# create word embedding by fetching each word vector\n",
    "for tok, idx in word2idx_in.items():\n",
    "    if idx < MAX_WORD_NUM:\n",
    "        word_vector = wordVec.get(tok)\n",
    "        if word_vector is not None:\n",
    "            word_embedding[idx] = word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create translated target matrix by loading the padded output target sequence using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_target_one_hot = np.zeros((len(eng), max_out_len, out_word_num), dtype='float32')\n",
    "\n",
    "for idx, tokVec in enumerate(man_seq_padded):\n",
    "    for tok_idx, tok in enumerate(tokVec):\n",
    "        if (tok > 0):\n",
    "            man_target_one_hot[idx, tok_idx, tok] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the encoder and decoder before attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yushuohan/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Input, LSTM, GRU, Dense, Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "# Embedding\n",
    "embedding = Embedding(wordNum, EMBEDDING, weights=[word_embedding], input_length=max_in_len)\n",
    "\n",
    "# Encoder\n",
    "input_layer_encoder = Input(shape=(max_in_len,))\n",
    "embed_encoder = embedding(input_layer_encoder)\n",
    "encoder = Bidirectional(LSTM(LATENT_DIM, return_sequences=True, dropout=0.5))\n",
    "encoder_out = encoder(embed_encoder)\n",
    "\n",
    "# Decoder input\n",
    "input_layer_decoder = Input(shape=(max_out_len,))\n",
    "embed_decoder = Embedding(out_word_num, EMBEDDING)\n",
    "decoder_input = embed_decoder(input_layer_decoder)\n",
    "\n",
    "# Decoder output, after attention\n",
    "decoder = LSTM(LATENT_DIM_DECODER, return_state=True)\n",
    "dense_decode = Dense(out_word_num, activation='softmax')\n",
    "s0 = Input(shape=(LATENT_DIM_DECODER,))\n",
    "c0 = Input(shape=(LATENT_DIM_DECODER,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Attention. \n",
    "First, the \"softmax over time\" activation function is implemented as follows: \n",
    "$\\alpha(t') = \\dfrac {exp(s(t'))}{\\sum_{x=1}^{|T_{x}|} exp(s(x)) }$. Context layer after all alpha attention weights are calculated is computed as follows: $context=\\sum_{t'=1}^{|T_{x}|} \\alpha(t')h(t')$. \n",
    "\n",
    "<img src=\"attention.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of x is N x T x D.\n",
    "def softmax(x):\n",
    "    assert(K.ndim(x) > 2)\n",
    "    e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
    "    s = K.sum(e, axis=1, keepdims=True)\n",
    "    return e / s\n",
    "\n",
    "# Some of the common layers for attention\n",
    "repeat_attn = RepeatVector(max_in_len)\n",
    "concat_attn = Concatenate(axis=-1)\n",
    "dense1_attn = Dense(10, activation='tanh')  # over time dimension T\n",
    "dense2_attn = Dense(1, activation=softmax)\n",
    "dot_attn = Dot(axes=1)                      # over time dimension T\n",
    "\n",
    "def iterAttn(h, prevOut):\n",
    "    \"\"\"\n",
    "    h: encoder encoded hidden states at all time.\n",
    "    prevOut: output at the previous time (word).\n",
    "    An iteration of attention.\n",
    "    \"\"\"\n",
    "    prevOutRepeat = repeat_attn(prevOut) # Tx, LATENT_DIM_DECODE\n",
    "    total = concat_attn([h, prevOutRepeat]) # Tx, LATENT_DIM_DECODE + LATENT_DIM * 2\n",
    "    d = dense1_attn(total)\n",
    "    alphaLayer = dense2_attn(d)\n",
    "    context = dot_attn([alphaLayer, h])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute encoder-decoder and attention (with teacher forcing) inference for $Ty$ times, to get a list of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s0\n",
    "c = c0\n",
    "\n",
    "# Iterate attention Ty times\n",
    "all_out = []\n",
    "for t in range(max_out_len):\n",
    "    # Get context vector with encoder and attention\n",
    "    context = iterAttn(encoder_out, s) \n",
    "    \n",
    "    # For teacher forcing, get the previous word\n",
    "    select_layer = Lambda(lambda x: x[:, t:t+1])\n",
    "    prevWord = select_layer(decoder_input)\n",
    "    \n",
    "    # Concat context and previous word as decoder input\n",
    "    concat2 = Concatenate(axis=2)\n",
    "    decoder_in_concat = concat2([context, prevWord])\n",
    "    \n",
    "    # pass into decoder, inference output\n",
    "    pred, s, c = decoder(decoder_in_concat, initial_state=[s, c])\n",
    "    pred = dense_decode(pred)\n",
    "    all_out.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output needs to be stacked to be considered as the network's output. Also, need batchsize N to be the first dimension, and thus a permutation of dimensions is required. Afterwards, the model can be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack(outputs):\n",
    "    outputs = K.stack(outputs)\n",
    "    return K.permute_dimensions(outputs, pattern=(1, 0, 2))\n",
    "\n",
    "stack_layer = Lambda(stack)\n",
    "all_out = stack_layer(all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attnModel = Model(inputs=[input_layer_encoder, input_layer_decoder, s0, c0,],\n",
    "                 outputs=all_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define customized loss and accuracy metrics for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myLoss(y_train, pred):\n",
    "    mask = K.cast(y_train > 0, dtype='float32')\n",
    "    val = mask * y_train * K.log(pred)\n",
    "    return -K.sum(val) / K.sum(mask)\n",
    "\n",
    "def acc(y_train, pred):\n",
    "    targ = K.argmax(y_train, axis=-1)\n",
    "    pred = K.argmax(pred, axis=-1)\n",
    "    correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
    "\n",
    "    mask = K.cast(K.greater(targ, 0), dtype='float32') # filter out padding value 0.\n",
    "    correctCount = K.sum(mask * correct)\n",
    "    totalCount = K.sum(mask)\n",
    "    return correctCount / totalCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model using Adam optimizer and defined loss and metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "attnModel.compile(optimizer='adam', loss=myLoss, metrics=[acc])\n",
    "\n",
    "# Define empty s0 and c0\n",
    "init_s = np.zeros((len(eng_seq_padded), LATENT_DIM_DECODER))\n",
    "init_c = np.zeros((len(eng_seq_padded), LATENT_DIM_DECODER))\n",
    "\n",
    "# Train\n",
    "history = attnModel.fit(\n",
    "    x=[eng_seq_padded, man_seq_padded, init_s, init_c],\n",
    "    y=man_target_one_hot,\n",
    "    batch_size=BATCHSIZE,\n",
    "    epochs=EPOCH,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['acc'], label='acc')\n",
    "plt.plot(history.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attnModel.save('attention_modeltmp.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in the inference model teacher forcing is not available, thus the model needs to be modified to use the previous inference result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inf = Model(input_layer_encoder, encoder_out)\n",
    "encoder_out_inf = Input(shape=(max_in_len, LATENT_DIM * 2,))\n",
    "\n",
    "# Decoder\n",
    "decoder_in_inf = Input(shape=(1,))\n",
    "decoder_in_embed_inf = embed_decoder(decoder_in_inf)\n",
    "\n",
    "# Context, concat without teacher forcing.\n",
    "context_inf = iterAttn(encoder_out_inf, s0)\n",
    "decoder_in_concat_inf = concat2([context, decoder_in_embed_inf])\n",
    "\n",
    "# Decoder decode\n",
    "pred, s, c = decoder(decoder_in_concat_inf, initial_state=[s0, c0])\n",
    "pred_out = dense_decode(pred)\n",
    "\n",
    "# Define model\n",
    "decoder_inf = Model(\n",
    "    inputs=[decoder_in_inf, encoder_out_inf, s0, c0],\n",
    "    outputs=[pred_out, s, c]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
